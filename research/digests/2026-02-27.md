# Daily arXiv Digest — LLM Agent Security (2026-02-27)

Selection window: **2026-02-26 13:30 UTC → 2026-02-27 13:30 UTC** (new + updated, including version/cross-list when present).  
Candidate pool in window: **16 papers** from configured feeds; selected: **10**.

**Filtering criteria (explicit):**
1. Priority to papers with direct overlap to **agent permissions/authorization/capability control**.
2. Then papers on **data security/privacy/exfiltration/prompt-injection-driven misuse**.
3. Then **memory-centric agent papers** (storage/retrieval/indexing/lifecycle) with clear operational/security implications.
4. Tie-breakers: recency, category overlap with `cs.CR/cs.AI/cs.CL/cs.LG/stat.ML`, and likely impact on near-term agent deployments.

---

## 1) A Decision-Theoretic Formalisation of Steganography With Applications to LLM Monitoring
- **Title / authors / arXiv ID (category):** *A Decision-Theoretic Formalisation of Steganography With Applications to LLM Monitoring* — Usman Anwar et al. — **arXiv:2602.23163** (**cs.CR**, cs.AI, cs.CL)
- **Summary (5–8 lines):**
  This paper reframes steganography for LLM oversight using a decision-theoretic lens rather than a reference-distribution lens.
  It introduces generalized V-information and the “steganographic gap” to quantify hidden-message usefulness to decoders vs non-decoders.
  The key contribution is operational: detection via downstream utility asymmetry, not token-level anomaly tests.
  Authors run empirical validations showing this formalism can detect and measure covert reasoning channels.
  They also discuss mitigation paths based on reducing utility asymmetry.
  Net result: a principled monitoring metric for covert communication risk in model outputs.
- **Novelty:**
  - Replaces distribution-matching steganography assumptions with utility-based detectability.
  - Introduces a quantifiable, deployment-facing “steganographic gap” metric.
- **Security relevance:**
  **Permissions/policy + data-security**: covert channels can bypass policy-level monitors and leak hidden intent/instructions despite surface-compliant outputs.
- **Limitations / open questions:**
  - Robustness of the metric under adaptive attackers optimizing against detector feedback.
  - Cost and calibration burden in real-time production monitoring pipelines.
- **Tags:** steganography, LLM monitoring, covert channels, oversight, evaluation metrics

## 2) Assessing Deanonymization Risks with Stylometry-Assisted LLM Agent
- **Title / authors / arXiv ID (category):** *Assessing Deanonymization Risks with Stylometry-Assisted LLM Agent* — Boyang Zhang, Yang Zhang — **arXiv:2602.23079** (**cs.CR**, cs.CL, cs.LG)
- **Summary (5–8 lines):**
  Proposes SALA, a stylometry-assisted LLM pipeline for authorship inference and privacy-risk assessment.
  The framework combines quantitative stylometric features with LLM reasoning for more interpretable attribution.
  A database-augmented variant improves inference performance across large-scale news settings.
  Beyond attack-side results, the paper adds mitigation via guided recomposition prompts to reduce identifiability.
  This gives a full attack-defense loop: infer identity risk, then rewrite while preserving semantic intent.
  Practical message: LLM agents can materially increase deanonymization risk if content pipelines are not privacy-audited.
- **Novelty:**
  - Hybrid stylometry + LLM reasoning for transparent attribution.
  - Built-in privacy mitigation loop (guided rewriting) instead of attack-only evaluation.
- **Security relevance:**
  **Data/privacy**: directly addresses inference-time privacy leakage and operational anonymization controls.
- **Limitations / open questions:**
  - Generalization to short/noisy text and multilingual domains.
  - Trade-off between anonymity gains and stylistic/semantic fidelity.
- **Tags:** privacy, deanonymization, stylometry, authorship inference, defense

## 3) CiteLLM: An Agentic Platform for Trustworthy Scientific Reference Discovery
- **Title / authors / arXiv ID (category):** *CiteLLM: An Agentic Platform for Trustworthy Scientific Reference Discovery* — Mengze Hong et al. — **arXiv:2602.23075** (cs.CL, cs.IR)
- **Summary (5–8 lines):**
  Introduces an agentic citation workflow embedded in LaTeX editing, focused on trustworthy retrieval and grounding.
  Architecture constrains retrieval to trusted academic repositories and uses LLMs for query/ranking/semantic validation.
  A key design claim is local workflow integration with no off-device data transmission.
  The system targets hallucination-free references using discipline-aware routing and paragraph-level support checks.
  Evaluation indicates improved validity and usability of returned references relative to baselines.
  It operationalizes “trustworthy assistant behavior” via bounded tool/data pathways.
- **Novelty:**
  - Strongly scoped retrieval boundaries in an agentic writing workflow.
  - Editor-native grounding pipeline balancing usability and integrity.
- **Security relevance:**
  **Permissions/data governance**: demonstrates least-exposure architecture (trusted sources + local-first handling) to reduce data leakage and integrity failures.
- **Limitations / open questions:**
  - Reliance on repository quality/coverage; weak for niche/gray literature.
  - Need adversarial testing against citation-poisoning and retrieval manipulation.
- **Tags:** trustworthy agents, retrieval grounding, privacy-preserving workflow, academic integrity, local-first

## 4) Managing Uncertainty in LLM-based Multi-Agent System Operation
- **Title / authors / arXiv ID (category):** *Managing Uncertainty in LLM-based Multi-Agent System Operation* — Man Zhang et al. — **arXiv:2602.23005** (cs.SE)
- **Summary (5–8 lines):**
  Focuses on system-level uncertainty rather than model-only uncertainty for multi-agent deployments.
  Distinguishes epistemic vs ontological uncertainty and maps them across runtime layers.
  Proposes a lifecycle framework: representation, identification, evolution, and adaptation.
  Uses a clinical multi-agent case to show improved diagnosability and operational reliability.
  The framing is relevant for software assurance where agent interactions amplify hidden uncertainty.
  It contributes governance structure for runtime control decisions in safety-critical contexts.
- **Novelty:**
  - Lifecycle treatment of uncertainty spanning orchestration, dataflow, and human-in-loop.
  - Runtime governance framing beyond model-centric calibration.
- **Security relevance:**
  **Capability/policy control**: uncertainty-aware runtime adaptation is critical when enforcing safe action boundaries under ambiguous states.
- **Limitations / open questions:**
  - Evidence primarily from domain-specific case study; broad external validity unproven.
  - Lacks explicit adversarial threat-model benchmarking.
- **Tags:** multi-agent governance, runtime assurance, uncertainty management, safety-critical systems, control loops

## 5) Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization
- **Title / authors / arXiv ID (category):** *Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization* — Zeyuan Liu et al. — **arXiv:2602.23008** (cs.AI, cs.LG)
- **Summary (5–8 lines):**
  Introduces EMPO², combining memory-augmented exploration with hybrid on/off-policy RL updates.
  Targets the exploration bottleneck in LLM agents operating in interactive environments.
  Reports strong gains on ScienceWorld and WebShop, plus better out-of-distribution adaptation.
  Notably claims robustness even when memory is unavailable during some evaluations.
  The paper is method-heavy but practically relevant for agents that must discover non-obvious states.
  It implies memory design choices directly affect both capability and safety envelope.
- **Novelty:**
  - Hybrid RL design balancing memory-dependent performance and memory-absent robustness.
  - Demonstrated OOD improvement with few memory-assisted trials.
- **Security relevance:**
  **Memory dimension**: stronger exploratory agents may access more sensitive states; memory policy and write/read controls become more important.
- **Limitations / open questions:**
  - Benchmarks are constrained environments; unclear behavior under adversarial tool ecosystems.
  - No direct analysis of memory poisoning or malicious retrieval artifacts.
- **Tags:** memory-augmented agents, RLHF alternatives, exploration, OOD adaptation, agent robustness

## 6) ESAA: Event Sourcing for Autonomous Agents in LLM-Based Software Engineering
- **Title / authors / arXiv ID (category):** *ESAA: Event Sourcing for Autonomous Agents in LLM-Based Software Engineering* — Elzo Brito dos Santos Filho — **arXiv:2602.23193** (cs.AI)
- **Summary (5–8 lines):**
  Proposes an event-sourced architecture separating agent “intent” from deterministic state mutation.
  Agents emit validated structured intents; orchestrator validates, persists append-only logs, and applies effects.
  Adds replay verification and hashing for traceability, plus boundary contracts for controlled actions.
  Two case studies (single-agent and multi-agent) report successful execution with verification checks.
  The architecture is essentially a governance wrapper around agent autonomy.
  It directly addresses long-horizon state drift and post-hoc accountability gaps.
- **Novelty:**
  - Event-sourcing pattern applied end-to-end for autonomous LLM agent orchestration.
  - Deterministic write-path with forensic replay guarantees.
- **Security relevance:**
  **Permissions/audit**: supports least-privilege execution and tamper-evident action history for policy enforcement.
- **Limitations / open questions:**
  - More architecture report than adversarial evaluation.
  - Performance overhead and failure handling at scale need deeper study.
- **Tags:** event sourcing, agent orchestration, auditability, deterministic execution, governance

## 7) ProactiveMobile: A Comprehensive Benchmark for Boosting Proactive Intelligence on Mobile Devices *(updated in window)*
- **Title / authors / arXiv ID (category):** *ProactiveMobile: A Comprehensive Benchmark for Boosting Proactive Intelligence on Mobile Devices* — Dezhi Kong et al. — **arXiv:2602.21858** (cs.AI)
- **Summary (5–8 lines):**
  Builds a benchmark for proactive mobile agents that infer latent intent and trigger executable API sequences.
  Covers 3,660+ instances across 14 scenarios with multi-answer annotations and expert audit.
  Formalizes proactive behavior from context signals into action plans over a 63-API function pool.
  Reported model results show current frontier systems still struggle on proactive intelligence.
  Although framed as capability benchmarking, it surfaces permission-sensitive mobile-action pathways.
  This update-version in the 24h window remains relevant for agent action governance research.
- **Novelty:**
  - Realistic proactive-task benchmark with executable action grounding.
  - Rich annotation + expert auditing for practical evaluation fidelity.
- **Security relevance:**
  **Permissions/capability control**: proactive API invocation intersects least-privilege design and consent boundaries on-device.
- **Limitations / open questions:**
  - Limited explicit security evaluation (e.g., abuse-resistant trigger conditions).
  - Needs adversarial prompt-injection/intent-spoof stress tests.
- **Tags:** proactive agents, mobile APIs, benchmark, intent inference, permission boundaries

## 8) STELLAR: Storage Tuning Engine Leveraging LLM Autonomous Reasoning for High Performance Parallel File Systems
- **Title / authors / arXiv ID (category):** *STELLAR: Storage Tuning Engine Leveraging LLM Autonomous Reasoning for High Performance Parallel File Systems* — Chris Egersdoerfer et al. — **arXiv:2602.23220** (cs.DC)
- **Summary (5–8 lines):**
  Presents an autonomous tuning agent for parallel file systems, using LLM-guided iterative optimization.
  Pipeline extracts parameters, analyzes traces, proposes tuning, executes, and updates strategy from feedback.
  Claims near-optimal tuning within very few attempts compared with conventional autotuners.
  Includes reflection/memory of prior tuning experiences to accelerate future tasks.
  Uses RAG + tools + multi-agent decomposition to stabilize decisions.
  Security-adjacent value is in controlled autonomous interaction with performance-critical infrastructure.
- **Novelty:**
  - Low-iteration autonomous tuning loop for complex storage systems.
  - Experience accumulation mechanism for reusable optimization memory.
- **Security relevance:**
  **Memory + capability governance**: autonomous infra tuning implies high-impact tool permissions; strong policy gates and audit controls are essential.
- **Limitations / open questions:**
  - Primarily performance-centric; security failure modes and abuse paths underexplored.
  - Unclear robustness when manuals/logs are adversarially manipulated.
- **Tags:** autonomous tuning, storage systems, tool-using agents, RAG, operational memory

## 9) LLM Novice Uplift on Dual-Use, In Silico Biology Tasks
- **Title / authors / arXiv ID (category):** *LLM Novice Uplift on Dual-Use, In Silico Biology Tasks* — Chen Bo Calvin Zhang et al. — **arXiv:2602.23329** (cs.CR, cs.AI, cs.CL)
- **Summary (5–8 lines):**
  Human study compares novice performance with LLM access vs internet-only baselines across biosecurity-relevant tasks.
  Reports large uplift (4.16x accuracy) and in some cases novice+LLM outperforming expert internet-only baselines.
  Also finds standalone models often exceed assisted-user outcomes, suggesting prompting/interaction bottlenecks.
  High participant-reported ease in obtaining dual-use-relevant info despite safeguards is notable.
  The work emphasizes interactive human-uplift testing over static benchmark scores.
  It shifts risk framing from model capability to user-capability amplification.
- **Novelty:**
  - Large multi-benchmark human-uplift design focused on dual-use contexts.
  - Quantifies practical access-to-capability transfer, not just model metrics.
- **Security relevance:**
  **Policy/access control**: raises urgency for capability-tiering and permission-aware deployment policies for sensitive domains.
- **Limitations / open questions:**
  - Domain-specific transferability beyond biology needs confirmation.
  - Guardrail efficacy analysis remains high-level; mechanism-level breakdown needed.
- **Tags:** dual-use risk, human uplift, biosecurity, access governance, deployment policy

## 10) Agency and Architectural Limits: Why Optimization-Based Systems Cannot Be Norm-Responsive
- **Title / authors / arXiv ID (category):** *Agency and Architectural Limits: Why Optimization-Based Systems Cannot Be Norm-Responsive* — Radha Sarma — **arXiv:2602.23239** (cs.AI, cs.CY)
- **Summary (5–8 lines):**
  Offers a formal argument that RLHF-style optimization systems cannot satisfy key conditions for robust norm responsiveness.
  Defines two required conditions (incommensurability and non-inferential suspension) and argues optimization conflicts with both.
  Interprets failures like sycophancy/unfaithful reasoning as structural rather than incidental.
  Introduces “convergence crisis” risk where human oversight degrades under metric pressure.
  The piece is theoretical but directly speaks to governance assumptions in high-stakes deployments.
  It is useful as a critique of purely objective-maximizing control paradigms.
- **Novelty:**
  - Architectural impossibility framing for norm governance in optimization-centric systems.
  - Cross-domain formalization linking governance failure modes to training objective structure.
- **Security relevance:**
  **Policy/capability**: challenges reliance on scalar optimization for enforcing hard safety boundaries in autonomous agents.
- **Limitations / open questions:**
  - Needs stronger empirical bridge from theory to deployed-system measurements.
  - May overgeneralize across heterogeneous alignment methods.
- **Tags:** AI governance, alignment theory, norm responsiveness, RLHF critique, safety architecture

---

## Top 3 must-read + recommended order
1. **2602.23163 (Steganography formalisation)** — Read first for a concrete, measurable oversight concept (“steganographic gap”) that could immediately inform monitoring design.
2. **2602.23193 (ESAA)** — Read second for actionable orchestration architecture (deterministic writes + append-only logs + replay verification) aligned with agent governance needs.
3. **2602.23079 (SALA deanonymization)** — Read third to cover privacy attack/defense practice and understand how agent reasoning can amplify identity leakage.

## 10-line trend summary
1. This 24h window had moderate volume but high concentration in **agent governance and oversight-adjacent** work.
2. Direct permission/authorization papers were fewer than expected; many contributions are architecture or monitoring proxies.
3. **Auditability-by-design** (event logs, deterministic orchestrators, replayability) is gaining traction as a practical control plane.
4. **Privacy risk research** is broadening from extraction attacks to authorship/deanonymization pipelines with built-in mitigation loops.
5. Monitoring is moving from surface heuristics toward **utility-aware detection** (e.g., steganographic asymmetry framing).
6. Memory-centered agent papers remain largely performance-oriented, with limited treatment of **memory poisoning/provenance**.
7. Updated benchmark work (e.g., proactive mobile agents) increases urgency around **permission-aware proactive actions**.
8. Human-uplift studies suggest risk scales with user enablement, not just raw model capability.
9. **Gap/opportunity #1:** standardized evaluations for least-privilege enforcement in tool-using/proactive agents are still missing.
10. **Gap/opportunity #2:** end-to-end memory security benchmarks (write provenance, retrieval integrity, lifecycle deletion guarantees) remain underdeveloped.
