# Daily arXiv Agent-Security Digest — 2026-02-24

Scope: papers **new or updated in last 24h** (cutoff: 2026-02-23 13:30 UTC → 2026-02-24 13:30 UTC), including version/cross-list updates when present.

Filtering criteria (18 candidates → 10 selected):
1. Keyword match strength for priority themes (permissions/policy/access control > data security/privacy/exfiltration > memory access/lifecycle).
2. Agent-security relevance (explicit attack/defense/evaluation/governance implications for LLM agents).
3. Potential impact (new benchmark, formalism, deployable system design, or broad operational takeaway).

---

## 1) Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks
- **Authors:** David Schmotz, Luca Beurer-Kellner, Sahar Abdelnabi, Maksym Andriushchenko  
- **arXiv:** 2602.20156v1 (**cs.CR**, cross-listed cs.LG)

### Summary
The paper studies a new supply-chain-like attack surface created by third-party skill files in agent ecosystems.  
It introduces **SkillInject**, a benchmark with 202 injection-task pairs spanning obvious and subtle/contextual payloads.  
The evaluation jointly measures security (harmful instruction refusal) and utility (ability to follow benign instructions).  
Across frontier models, attack success can reach ~80%, with harmful outcomes including exfiltration and destructive behavior.  
The authors show model scaling alone and simple filtering are insufficient to eliminate these failures.  
They argue for context-aware authorization mechanisms rather than prompt-only defenses.

### Novelty
- First dedicated benchmark targeting **skill-file prompt injection** in agent pipelines.  
- Empirical evidence that utility-security trade-offs remain severe even in stronger models.

### Security relevance
- **Permissions/policy dimension:** Directly motivates capability-gated and provenance-aware authorization for skills/tools.  
- Demonstrates that untrusted skill content can bypass naive guardrails and steer privileged actions.

### Limitations / open questions
- Benchmark coverage may not reflect all real-world agent orchestration patterns.  
- Needs longitudinal evaluation with adaptive defenses and policy engines in-the-loop.  
- Unclear how transferably the findings map across closed-source production agents.

### Tags
`prompt-injection` `agent-skills` `authorization` `tool-supply-chain` `benchmark`

---

## 2) The LLMbda Calculus: AI Agents, Conversations, and Information Flow
- **Authors:** Zac Garby, Andrew D. Gordon, David Sands  
- **arXiv:** 2602.20064v1 (**cs.PL**, cross-listed cs.AI/cs.CR)

### Summary
This work proposes a formal calculus for prompt-response conversations with tool calls and planner loops.  
It models LLM invocation as a first-class primitive, making conversation state explicit in semantics.  
The framework captures how injected prompts influence downstream computation and tool execution.  
Dynamic information-flow control is integrated to reason about confidentiality and integrity constraints.  
The paper proves a termination-insensitive noninterference theorem under its formal setting.  
It also frames practical defenses (quarantine conversations, code isolation, influence restrictions) in rigorous terms.

### Novelty
- A principled **programming-language semantics** for agent conversations + LLM calls.  
- Formal noninterference guarantees adapted to agentic prompt/tool workflows.

### Security relevance
- **Permissions/policy dimension:** Offers a formal basis for capability boundaries and policy enforcement.  
- Could inform static/runtime checks for what data may influence high-risk tool calls.

### Limitations / open questions
- Formal abstractions may omit deployment details (latency, multi-model chains, retrieval noise).  
- Need validation that theorem-backed constraints remain practical under real agent workloads.

### Tags
`formal-methods` `information-flow` `agent-security` `prompt-injection` `noninterference`

---

## 3) LLM-enabled Applications Require System-Level Threat Monitoring
- **Authors:** Yedi Zhang, Haoyu Wang, Xianglin Yang, Jin Song Dong, Jun Sun  
- **arXiv:** 2602.19844v1 (**cs.CR**, cross-listed cs.AI/cs.SE)

### Summary
This position paper argues that LLM-system failures should be treated as expected operational conditions.  
The authors emphasize that model-level guardrails and pre-deployment testing are insufficient alone.  
They advocate system-level, post-deployment threat monitoring as a reliability prerequisite.  
The framing is incident-response oriented: detect anomalies, contextualize them, and feed them into response loops.  
A core claim is that deployment trust hinges more on observability architecture than marginal model improvements.  
The paper calls for comprehensive threat telemetry across the full application stack.

### Novelty
- Strong operational-security framing: from “one-time eval” to continuous incident-oriented monitoring.  
- Consolidates reliability and security into a unified deployment doctrine for LLM apps.

### Security relevance
- **Data/permissions dimension:** Monitoring is essential to catch policy bypass, data leaks, and unsafe tool activations in production.  
- Useful for building SOC-like workflows around agent systems.

### Limitations / open questions
- Mostly conceptual; lacks concrete implementation blueprint and benchmark metrics.  
- Trade-offs among observability depth, privacy, and cost remain underspecified.

### Tags
`runtime-monitoring` `incident-response` `observability` `agent-ops` `threat-detection`

---

## 4) MAS-FIRE: Fault Injection and Reliability Evaluation for LLM-Based Multi-Agent Systems
- **Authors:** Jin Jia, Zhiling Deng, Zhuangbin Chen, Yingqi Wang, Zibin Zheng  
- **arXiv:** 2602.19843v1 (**cs.SE**, cross-listed cs.AI)

### Summary
MAS-FIRE introduces a fault-injection framework to probe failure and recovery in multi-agent systems.  
It defines 15 fault types spanning intra-agent cognitive errors and inter-agent coordination failures.  
Faults are injected through prompt edits, response rewriting, and message-routing manipulation.  
The study compares architectures and identifies four tiers of resilience mechanisms (mechanism/rule/prompt/reasoning).  
Findings show stronger base models do not guarantee stronger robustness under injected faults.  
Iterative closed-loop topologies neutralize many failures that collapse linear workflows.

### Novelty
- A structured fault taxonomy + non-invasive injection protocol for MAS reliability/security testing.  
- Process-level diagnostics beyond coarse task-success metrics.

### Security relevance
- **Permissions/policy dimension (indirect):** Helps expose control-plane fragility where policy assumptions silently fail.  
- Useful for stress-testing agent orchestration against adversarial perturbations.

### Limitations / open questions
- Not focused specifically on confidentiality/integrity leak outcomes.  
- External validity depends on representativeness of injected fault models.

### Tags
`multi-agent` `fault-injection` `robustness` `evaluation` `architecture-security`

---

## 5) Assessing Risks of LLMs in Mental Health Support: Automated Clinical AI Red Teaming
- **Authors:** Ian Steenstra, Paola Pedrelli, Weiyan Shi, Stacy Marsella, Timothy W. Bickmore  
- **arXiv:** 2602.19948v1 (**cs.CL**, cross-listed cs.AI/cs.CY/cs.HC/cs.MA)

### Summary
The paper builds a simulation-based red-teaming framework for longitudinal therapeutic dialogues.  
It pairs AI therapist agents with dynamic simulated patients and evaluates care-quality and risk ontology outcomes.  
In 369 simulated sessions across diverse personas, multiple systems show critical safety failures.  
Reported risks include reinforcing delusions and poor de-escalation in suicidal-risk contexts.  
The framework includes dashboard support for engineers, clinicians, and policy stakeholders.  
The main claim: pre-deployment testing must include longitudinal, clinically grounded adversarial simulation.

### Novelty
- Domain-specific red-team methodology capturing **long-horizon conversational risk**.  
- Stakeholder-facing audit workflow bridging technical and clinical oversight.

### Security relevance
- **Policy/governance dimension:** Extends red teaming to sustained interaction risks, not single-turn jailbreaks.  
- Supports auditability for high-risk agent deployments.

### Limitations / open questions
- Domain-specific findings may not transfer directly to non-clinical agent settings.  
- Simulation realism and ontology coverage could bias measured failure prevalence.

### Tags
`red-teaming` `longitudinal-risk` `clinical-ai` `safety-audit` `agent-evaluation`

---

## 6) BarrierSteer: LLM Safety via Learning Barrier Steering
- **Authors:** Thanh Q. Tran, Arun Verma, Kiwan Wong, Bryan Kian Hsiang Low, Daniela Rus, Wei Xiao  
- **arXiv:** 2602.20102v1 (**cs.LG**, cross-listed cs.AI)

### Summary
BarrierSteer proposes latent-space safety steering using control barrier functions (CBFs).  
The method detects unsafe generation trajectories and steers outputs at inference time.  
It supports multiple constraints through merged enforcement, without retraining model parameters.  
The paper claims theoretical guarantees around efficiency/principled constraint handling.  
Experiments report lower adversarial success and fewer unsafe generations than several baselines.  
The approach targets safety-performance balance while preserving core capability.

### Novelty
- Control-theoretic CBF framing for LLM response steering in latent space.  
- Multi-constraint safety enforcement without fine-tuning base weights.

### Security relevance
- **Policy dimension:** Potentially useful as a runtime safety layer for dangerous-action suppression.  
- Could complement authorization by reducing unsafe output trajectories pre-tool execution.

### Limitations / open questions
- Needs validation against adaptive, tool-mediated agent attacks (not just text-generation settings).  
- Potential brittleness if latent constraints are mis-specified.

### Tags
`safety-steering` `control-barrier-functions` `inference-time-defense` `adversarial-robustness` `guardrails`

---

## 7) Rethinking LoRA for Privacy-Preserving Federated Learning in Large Models
- **Authors:** Jin Liu, Yinbin Miao, Ning Xi, Junkang Liu  
- **arXiv:** 2602.19926v1 (**cs.LG**, cross-listed cs.AI)

### Summary
This paper analyzes why vanilla LoRA underperforms in differentially private federated learning (DPFL).  
It identifies gradient coupling, DP-noise amplification, and sharp global-model landscapes as key causes.  
The proposed LA-LoRA alternates local updates to decouple interactions and stabilize learning under privacy noise.  
The authors provide convergence arguments for noisy federated settings.  
Empirically, LA-LoRA improves utility under strict privacy budgets on both vision and language backbones.  
The work contributes practical tuning guidance for privacy-constrained large-model adaptation.

### Novelty
- Diagnoses LoRA-specific failure modes under DPFL rather than treating PEFT as plug-and-play.  
- Introduces alternating-update design aligned with privacy-noise robustness.

### Security relevance
- **Data privacy/governance dimension:** Strengthens private training pipelines for large models under federated constraints.  
- Relevant to reducing leakage risk in distributed adaptation workflows.

### Limitations / open questions
- Focuses on training privacy, not inference-time exfiltration or agent runtime policy.  
- Needs broader replication across heterogeneous client/data distributions.

### Tags
`differential-privacy` `federated-learning` `LoRA` `data-governance` `PEFT`

---

## 8) An Explainable Memory Forensics Approach for Malware Analysis
- **Authors:** Silvia Lucia Sanna, Davide Maiorca, Giorgio Giacinto  
- **arXiv:** 2602.19831v1 (**cs.CR**)

### Summary
The paper addresses interpretability and analyst burden in memory-forensics workflows.  
It applies LLMs to interpret Volatility-like outputs and extract indicators of compromise (IoCs).  
The method compares full RAM acquisition and process-memory dumping for complementary evidence value.  
A human-in-the-loop approach is emphasized for reproducibility and analyst oversight.  
Results suggest improved IoC extraction in some settings relative to prior tools.  
The practical contribution is explainable, AI-assisted malware triage over volatile artifacts.

### Novelty
- Couples memory-forensics pipelines with explainable LLM-assisted interpretation.  
- Demonstrates dual-mode acquisition analysis (full RAM + process dump) in one workflow.

### Security relevance
- **Memory/access dimension:** Relevant to secure memory artifact handling and evidence interpretation pipelines.  
- Less agent-policy specific, but strong on memory lifecycle/analysis operations.

### Limitations / open questions
- Potential LLM hallucination risk in forensic interpretation.  
- Requires rigorous chain-of-custody and validation protocols for legal/incident contexts.

### Tags
`memory-forensics` `malware-analysis` `IoC-extraction` `human-in-the-loop` `explainability`

---

## 9) AgenticSum: Agentic Inference-Time Framework for Faithful Clinical Text Summarization
- **Authors:** Fahmida Liza Piya, Rahmatollah Beheshti  
- **arXiv:** 2602.20040v1 (**cs.CL**, cross-listed cs.AI)

### Summary
AgenticSum decomposes summarization into staged context selection, generation, verification, and targeted correction.  
The architecture uses internal grounding signals to flag weakly supported spans for repair.  
It reports consistent gains over vanilla LLM baselines on public clinical datasets.  
The method is inference-time oriented, avoiding heavy retraining while improving factual faithfulness.  
Its main practical value is reducing unsupported statements in high-stakes documentation settings.  
The staged design also improves interpretability of error-localization steps.

### Novelty
- Modular, correction-centric inference-time agentic summarization loop.  
- Selective revision mechanism focused on weak evidence spans.

### Security relevance
- **Data integrity/audit dimension:** Improves traceability and faithfulness in sensitive medical text processing.  
- Not a direct permission-control paper, but useful for trustworthy data handling pipelines.

### Limitations / open questions
- Primarily quality-focused; does not directly model adversarial prompt/tool threats.  
- Generalization beyond clinical domains remains to be shown.

### Tags
`agentic-pipeline` `faithfulness` `clinical-nlp` `verification` `data-integrity`

---

## 10) Unlocking Multimodal Document Intelligence: Future Frontiers of Visual Document Retrieval
- **Authors:** Yibo Yan et al.  
- **arXiv:** 2602.19961v1 (**cs.CL**, cross-listed cs.IR)

### Summary
This is a broad survey of visual document retrieval (VDR) in the MLLM era.  
It maps benchmarks and method families across embedding, reranking, and RAG/agentic integration.  
The paper emphasizes document-specific challenges: dense text, complex layouts, and semantic dependencies.  
It identifies persistent gaps and open directions for multimodal document intelligence.  
Security is not central, but retrieval architecture choices affect access pathways and provenance control.  
The survey is useful as context for memory/retrieval-layer governance discussions.

### Novelty
- First comprehensive VDR survey explicitly through the MLLM/agentic lens.  
- Consolidates benchmark/task taxonomy for multimodal retrieval pipelines.

### Security relevance
- **Memory/retrieval dimension:** Helps frame where retrieval indexing/access controls should be inserted in multimodal stacks.  
- Indirect but relevant for secure knowledge access architecture.

### Limitations / open questions
- Survey paper (no new empirical defense technique).  
- Security/privacy treatment is peripheral rather than systematic.

### Tags
`multimodal-retrieval` `RAG` `document-intelligence` `indexing` `survey`

---

## Top 3 must-read + recommended reading order
1. **Skill-Inject (2602.20156v1)** — Start here for the clearest empirical signal that current agents fail badly on skill-file injections; immediate implications for authorization design.  
2. **LLMbda Calculus (2602.20064v1)** — Then read for rigorous foundations on information flow and principled policy boundaries in agent loops.  
3. **System-Level Threat Monitoring (2602.19844v1)** — Finish with operationalization guidance: how to monitor and respond once systems are deployed.

## 10-line trend summary
1. Prompt-injection risk is moving from plain prompts to **supply-chain surfaces** (skills/tools).  
2. Benchmarks are becoming more realistic, with subtle/contextual attacks rather than only obvious jailbreak strings.  
3. There is growing recognition that **authorization/context policy** must complement model-level safety tuning.  
4. Formal methods are re-entering the field, offering integrity/confidentiality guarantees for agent conversations.  
5. Reliability frameworks (fault injection, topology analysis) show architecture can matter as much as model strength.  
6. Runtime observability is emerging as a central requirement; static guardrails alone are no longer considered enough.  
7. Privacy-preserving training research remains active, but often disconnected from runtime agent exfiltration threats.  
8. Memory-focused works are increasing, especially around interpretation, retrieval, and lifecycle tooling in high-stakes domains.  
9. **Gap/opportunity #1:** unify formal policy guarantees with practical runtime monitors and enforcement hooks.  
10. **Gap/opportunity #2:** build end-to-end evaluations combining permissions, retrieval/memory access control, and adversarial tool ecosystems.
