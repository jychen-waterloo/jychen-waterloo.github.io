# Daily arXiv Patrol — LLM Agent Security (2026-02-26)

**Window scanned:** 2026-02-25 13:30 UTC → 2026-02-26 13:30 UTC  
**Sources:** arXiv RSS (`cs.CR`, `cs.AI`, `cs.CL`, `cs.LG`, `stat.ML`) + arXiv API keyword feeds from `research/rss_feeds.md`  
**Filtering criteria (top 10 selected from a larger candidate pool):**
1. Direct match to prioritized themes (permissions/access control > data security/privacy > memory access/lifecycle).
2. Explicit agent/tool/orchestrator threat model or defense method (not generic LLM benchmarking).
3. Potential impact on real deployments (attack success, measurable leakage reduction, or operationally actionable design).
4. Inclusion of **new papers + updated versions/cross-lists** inside the 24h window.

---

## 1) Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks
- **Authors:** David Schmotz, Luca Beurer-Kellner, Sahar Abdelnabi, Maksym Andriushchenko
- **arXiv:** `2602.20156v3` (`cs.CR`, `cs.LG`)

### Summary (5–8 lines)
- The paper studies a new supply-chain attack surface created by third-party “skills” in agent systems.
- It introduces **SkillInject**, a benchmark with 202 injection-task pairs ranging from obvious to subtle context-dependent payloads hidden in skill files.
- Evaluation across frontier agents shows high vulnerability, with attack success rates reaching ~80%.
- Attacks induce severe outcomes, including data exfiltration and destructive/ransomware-like behavior.
- Results show that larger model scale alone does not reliably reduce risk.
- Simple filters are insufficient because malicious instructions can be entangled with legitimate skill functionality.
- The authors argue robust, context-aware authorization is required at skill execution boundaries.

### Novelty
- First focused benchmark on **skill-file prompt injection** as an agent supply-chain problem.
- Separates utility and security metrics to expose tradeoffs in practical agent deployments.

### Security relevance
- **Permissions/capability policy:** Highlights missing authorization checks between skill intent and effective action permissions.
- Demonstrates why least-privilege needs to be enforced at tool/skill invocation time, not only at prompt parsing.

### Limitations / open questions
- Benchmark-centered evidence; transferability to all production agent stacks remains to be validated.
- Defense proposals are directional; full reference architecture for safe skill ecosystems is still open.

### Tags
`agent-security`, `prompt-injection`, `skill-supply-chain`, `authorization`, `least-privilege`

---

## 2) OMNI-LEAK: Orchestrator Multi-Agent Network Induced Data Leakage
- **Authors:** Akshat Naik, Jay Culligan, Yarin Gal, Philip Torr, Rahaf Aljundi, Alasdair Paren, Adel Bibi
- **arXiv:** `2602.13477v2` (`cs.AI`)

### Summary (5–8 lines)
- This work examines orchestrator-style multi-agent systems where a central coordinator delegates to specialized agents.
- The authors show **OMNI-LEAK**, an indirect prompt-injection vector that compromises multiple agents and leaks sensitive data.
- Importantly, leakage occurs even when baseline data access controls are present.
- They test attack categories against frontier reasoning and non-reasoning models and find both vulnerable.
- The attacker does not require full insider knowledge of implementation details.
- Findings suggest multi-agent composition introduces emergent leakage channels absent in single-agent evaluations.

### Novelty
- Moves threat modeling from single-agent setups to orchestrated multi-agent networks with delegation pathways.
- Demonstrates practical leakage despite nominal access control mechanisms.

### Security relevance
- **Data security/privacy + permissions:** Shows that per-agent ACLs are insufficient without cross-agent flow controls and delegation-aware policy enforcement.

### Limitations / open questions
- Based on representative orchestrator patterns; generalization to all architectures may vary.
- Needs standardized benchmarks for compositional policy verification across agents.

### Tags
`multi-agent`, `orchestrator`, `data-leakage`, `indirect-prompt-injection`, `access-control`

---

## 3) Bypassing AI Control Protocols via Agent-as-a-Proxy Attacks
- **Authors:** Jafar Isbarov, Murat Kantarcioglu
- **arXiv:** `2602.05066v2` (`cs.CR`, `cs.AI`)

### Summary (5–8 lines)
- The paper challenges monitoring-based defenses that inspect chain-of-thought plus tool actions for policy violations.
- It proposes an **Agent-as-a-Proxy** attack where injected instructions turn the agent into a covert relay.
- On AgentDojo, the method bypasses both AlignmentCheck and Extract-and-Evaluate style monitors.
- Critically, bypasses remain effective even against large monitoring models.
- The work argues monitor scale does not guarantee robustness against adaptive, path-dependent attack strategies.
- This weakens confidence in “just add a stronger overseer model” as a primary defense.

### Novelty
- Formalizes a proxy-style bypass that targets the oversight architecture itself.
- Provides empirical evidence that frontier-scale monitors can still fail under crafted indirection.

### Security relevance
- **Permissions/policy enforcement:** Monitoring-only safety controls are brittle without hard execution constraints and provenance checks.

### Limitations / open questions
- Focuses on selected monitoring protocols; broader protocol families should be tested.
- More work needed on defenses combining static policy gates with runtime taint/provenance.

### Tags
`oversight`, `monitor-bypass`, `agent-as-proxy`, `policy-enforcement`, `tool-security`

---

## 4) Beyond Refusal: Probing the Limits of Agentic Self-Correction for Semantic Sensitive Information
- **Authors:** Umid Suleymanov, Zaur Rajabov, Emil Mirzazada, Murat Kantarcioglu
- **arXiv:** `2602.21496v1` (`cs.AI`)

### Summary (5–8 lines)
- Targets leakage of **semantic sensitive information** (inferred attributes, harmful semantic disclosures), beyond structured PII.
- Introduces SemSIEdit, where an editor agent critiques/rewrites sensitive spans instead of refusing output.
- Reports a privacy–utility frontier: substantial leakage reduction with relatively modest utility loss.
- Finds scale-dependent behavior: larger models can add contextual nuance, while smaller ones truncate destructively.
- Identifies a “reasoning paradox”: stronger reasoning increases baseline inference risk but also improves defensive rewriting.

### Novelty
- Frames semantic privacy as a controllable inference-time rewriting problem, not only refusal classification.
- Exposes model-scale differences in how privacy-preserving edits are realized.

### Security relevance
- **Data privacy/governance:** Useful for post-generation sanitization pipelines where hard refusals are impractical.

### Limitations / open questions
- Inference-time editing may still leak in edge contexts requiring domain-specific policy definitions.
- Risk of overfitting to benchmark semantics vs real operational policy regimes.

### Tags
`semantic-privacy`, `inference-time-defense`, `sensitive-information`, `utility-tradeoff`, `agentic-editing`

---

## 5) 1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent Reasoning
- **Authors:** Wenkai Li, Liwen Sun, Zhenxiang Guan, Xuhui Zhou, Maarten Sap
- **arXiv:** `2508.07667v3` (`cs.AI`)

### Summary (5–8 lines)
- Addresses contextual privacy in mixed public/private input settings such as meeting summarization.
- Uses a multi-agent decomposition (e.g., extraction + classification + validation) to reduce single-model cognitive overload.
- Systematic ablations on information-flow topology show where privacy errors originate and cascade.
- Demonstrates notable leakage reduction on ConfAIde and PrivacyLens while preserving public-content fidelity.
- Updated version remains relevant because it sharpens practical multi-agent privacy design patterns.

### Novelty
- Connects privacy failures to **information-flow topology**, not just model accuracy.
- Provides actionable decomposition patterns for privacy-aware generation pipelines.

### Security relevance
- **Data privacy/access governance:** Supports policy-aligned partitioning of sensitive context across cooperative agents.

### Limitations / open questions
- Multi-agent complexity may increase operational overhead and failure debugging burden.
- Transferability to multilingual, domain-regulated settings remains open.

### Tags
`contextual-privacy`, `multi-agent-reasoning`, `information-flow`, `privacy-leakage`, `governance`

---

## 6) ImpMIA: Leveraging Implicit Bias for Membership Inference Attack
- **Authors:** (as listed in arXiv) 
- **arXiv:** `2510.10625` (`cs.LG`, `cs.CR`, `cs.CV`)

### Summary (5–8 lines)
- Studies membership inference by exploiting implicit model biases rather than only confidence-based signals.
- Suggests latent representational cues can leak whether samples were seen during training.
- Extends privacy risk analysis beyond direct extraction and confidence calibration attacks.
- Though not strictly agent-focused, this risk propagates into agentic systems that expose model judgments through tool APIs.
- Relevant to governance policies for training data handling and release-time audit requirements.

### Novelty
- Uses implicit bias channels as inference vectors for membership attacks.
- Expands the attack surface model for modern LLM-integrated systems.

### Security relevance
- **Data governance/privacy audit:** Informs model release audits and risk scoring when agents mediate sensitive queries.

### Limitations / open questions
- Need stronger evidence on impact in large agentic production stacks.
- Defense implications for aligned/finetuned models remain underexplored.

### Tags
`membership-inference`, `privacy-attack`, `training-data-governance`, `audit`, `model-risk`

---

## 7) Structurally Aligned Subtask-Level Memory for Software Engineering Agents
- **Authors:** Kangning Shen, Jingyuan Zhang, Chenxi Sun, Wencong Zeng, Yang Yue
- **arXiv:** `2602.21611v1` (`cs.SE`, `cs.AI`)

### Summary (5–8 lines)
- Diagnoses a granularity mismatch in agent memory: storing full episodes can cause retrieval of superficially similar but logically irrelevant traces.
- Proposes subtask-level memory aligned with functional decomposition of software-agent workflows.
- Aligns storage, retrieval, and update operations at subtask boundaries.
- Reports consistent Pass@1 gains on SWE-bench Verified against vanilla and instance-level memory baselines.
- Gains increase on longer trajectories, suggesting better lifecycle management of reusable reasoning fragments.

### Novelty
- Introduces structural alignment between agent decomposition and memory indexing units.
- Empirically links memory granularity choice to long-horizon performance.

### Security relevance
- **Memory access/lifecycle:** Better retrieval precision can reduce accidental exposure or misuse of irrelevant historical context.

### Limitations / open questions
- Focused on SWE tasks; security-specific memory contamination testing is still needed.
- Additional controls needed for provenance and trust scoring of stored subtasks.

### Tags
`agent-memory`, `subtask-indexing`, `retrieval`, `lifecycle-management`, `software-agents`

---

## 8) MemoPhishAgent: Memory-Augmented Multi-Modal LLM Agent for Phishing URL Detection
- **Authors:** Xuan Chen, Hao Liu, Yuan Tao, Mehran Kafai, Piotr Habas, Xiangyu Zhang
- **arXiv:** `2602.21394v1` (`cs.CR`)

### Summary (5–8 lines)
- Presents a phishing-detection agent combining multimodal tool orchestration with episodic memory.
- Shows strong recall gains over prompt-only baselines on public and real-world social-media URL sets.
- Ablations attribute substantial gains to memory-assisted reuse of prior reasoning trajectories.
- Includes deployment evidence at production scale, suggesting practical operational viability.
- While primarily defensive cybersecurity, it provides useful evidence on memory utility under adversarial drift.

### Novelty
- Integrates episodic memory into phishing toolchains with measurable production-scale impact.
- Demonstrates memory contribution under realistic evolving attack distributions.

### Security relevance
- **Memory/security intersection:** Illustrates how memory can improve adaptive detection—but also raises need for memory integrity controls.

### Limitations / open questions
- Domain-specific tooling may not transfer directly to general-purpose agent frameworks.
- Memory poisoning resilience is not deeply analyzed.

### Tags
`phishing-detection`, `memory-augmented-agent`, `cybersecurity`, `multimodal`, `deployment`

---

## 9) Structured Prompt Language: Declarative Context Management for LLMs
- **Authors:** Wen G. Gong
- **arXiv:** `2602.21257v1` (`cs.CL`, `cs.DB`, `cs.PL`)

### Summary (5–8 lines)
- Introduces SPL, a declarative language for explicit context budgeting and query planning in LLM pipelines.
- Adds SQL-like transparency (`EXPLAIN`) and explicit token/resource constraints.
- Integrates retrieval and persistent memory into one programmable abstraction.
- Includes fallback/routing mechanisms for resilient multi-provider execution.
- Not security-first, but highly relevant to controllable context access and memory/query lifecycle governance.

### Novelty
- Treats prompts/context as a managed data system with declarative controls.
- Bridges LLM orchestration, memory handling, and execution transparency in one DSL.

### Security relevance
- **Permissions/memory governance:** Explicit budgeting and transparent plans can support auditable least-privilege context exposure.

### Limitations / open questions
- Security guarantees are implied rather than formally proved.
- Requires empirical evaluation under adversarial prompt-injection and tainted retrieval settings.

### Tags
`context-management`, `declarative-control`, `memory-querying`, `auditability`, `llm-pipelines`

---

## 10) Budget-Aware Agentic Routing via Boundary-Guided Training
- **Authors:** Caiqi Zhang et al.
- **arXiv:** `2602.21227v1` (`cs.CL`, `cs.AI`)

### Summary (5–8 lines)
- Formulates agentic model routing as sequential decision-making under strict task-level budget constraints.
- Uses boundary policies (always-small vs always-large) to guide training and stabilize sparse-reward learning.
- Optimizes cost-success frontier for long-horizon workflows.
- Though cost-centric, it matters for security operations where budget pressure influences when safeguards are invoked.
- Offers a practical framework to reason about capability allocation during multi-step execution.

### Novelty
- Boundary-guided training objective for budget-constrained, path-dependent routing decisions.
- Moves beyond single-turn routing to long-horizon agent trajectories.

### Security relevance
- **Capability policy:** Enables policy designs where stronger models are reserved for high-risk steps requiring stricter safety checks.

### Limitations / open questions
- Direct security outcomes are not benchmarked.
- Need coupling with risk-aware triggers and policy calibration for real deployments.

### Tags
`agent-routing`, `capability-allocation`, `budget-policy`, `long-horizon`, `risk-aware-control`

---

## Top 3 Must-Read + Recommended Order

1. **Skill-Inject (2602.20156v3)**  
   Start here for the clearest near-term risk: third-party skill ecosystems create a prompt-injection supply chain with high measured success rates.

2. **OMNI-LEAK (2602.13477v2)**  
   Read second to understand why single-agent safety intuition fails in orchestrated multi-agent systems, even with basic access controls.

3. **Agent-as-a-Proxy (2602.05066v2)**  
   Read third to see why monitor-centric defenses alone are brittle and why enforcement must shift toward hard policy gates + provenance.

---

## 10-Line Trend Summary (with gaps/opportunities)
1. The last-24h update stream is dominated by **agent integration risk**, not base-model-only vulnerabilities.
2. Prompt-injection threat modeling is moving from chat prompts to **supply chain artifacts** (skills, tool wrappers, orchestration glue).
3. Multi-agent architectures are showing emergent leakage paths that are not captured by single-agent evaluations.
4. Monitoring/oversight-only defenses are repeatedly bypassed, indicating limited security from model scaling alone.
5. Privacy work is shifting from structured PII to harder **semantic leakage** and contextual exposure control.
6. Memory is transitioning from “performance feature” to “security-relevant subsystem” due to retrieval and persistence side effects.
7. Declarative context/memory management is an emerging direction for auditability and reproducible policy enforcement.
8. **Gap #1:** Lack of standardized benchmarks for cross-agent permission propagation and end-to-end data-flow taint tracking.
9. **Gap #2:** Limited formal guarantees for memory integrity/provenance under adversarial writes and retrieval poisoning.
10. Opportunity: combine capability-aware routing + authorization-aware tool execution + memory provenance into a unified agent control plane.
