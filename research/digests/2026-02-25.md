# Daily arXiv Agent-Security Digest — 2026-02-25

## Scope & filtering
- **Window:** 2026-02-24 13:30 UTC → 2026-02-25 13:30 UTC (papers **new or updated**, including cross-list/version updates).
- **Sources used:** `cs.CR`, `cs.AI`, `cs.CL`, `cs.LG`, `stat.ML` RSS + targeted arXiv API keyword feeds from `research/rss_feeds.md`.
- **Selection rule (12 of 481 candidates):** ranked by (1) direct match to **agent permissions/authorization/access control**; then (2) **data security/privacy/exfiltration/audit**; then (3) **memory storage/access/query/index lifecycle**; plus practical impact on deployable agent stacks.
- **Filtering criteria:** keyword match + explicit agent-security mechanism + measured/evaluable contribution; deprioritized unrelated agent capability papers.

---

## 1) OpenPort Protocol: A Security Governance Specification for AI Agent Tool Access
- **Authors:** Genliang Zhu, Chu Wang, Ziyuan Wang, Zhida Li, Qiang Li
- **arXiv:** `2602.20196v1` (**cs.CR**, cs.AI)

### Summary
OpenPort proposes a governance-first protocol for agent tool access instead of ad-hoc tool wrappers.
It formalizes authorization-dependent tool discovery and scoped permissions with ABAC-style constraints.
For write actions, it introduces a risk-gated lifecycle (draft by default, review, bounded auto-exec).
It explicitly addresses TOCTOU drift through a “State Witness” revalidation profile before delayed execution.
The protocol standardizes stable error semantics (`agent.*` reason codes) for deterministic recovery behavior.
Operational controls include quota/rate admission and structured audit events across allow/deny/fail outcomes.
The paper backs this with a reference runtime plus conformance, negative-security, and fuzz/regression tooling.

### Novelty
- Unified protocol-level treatment of authorization, write-risk gating, and audit semantics in one spec.
- Explicit TOCTOU mitigation profile for delayed approval workflows.

### Security relevance
- **Permissions:** strongest paper in-window on least privilege, policy constraints, and safe execution gating.
- **Data/audit:** structured logs + deterministic failure semantics improve forensic accountability.

### Limitations / open questions
- Real-world interoperability burden with incumbent tool APIs is not deeply quantified.
- Policy authoring complexity (ABAC correctness, conflict resolution) needs usability data.

### Tags
`agent-security` `authorization` `least-privilege` `tool-governance` `auditability` `protocol`

---

## 2) Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks
- **Authors:** David Schmotz, Luca Beurer-Kellner, Sahar Abdelnabi, Maksym Andriushchenko
- **arXiv:** `2602.20156v2` (**cs.CR**, cs.LG) — **version update in window**

### Summary
Skill-Inject introduces a benchmark focused on prompt-injection payloads hidden in third-party skill files.
It contains 202 attack-task pairs spanning obvious to subtle, context-dependent malicious instructions.
Evaluation of frontier models shows high attack success rates (up to ~80% in reported settings).
Observed failures include severe outcomes: data exfiltration, destructive actions, ransomware-like behavior.
The study measures both security (harmful compliance) and utility (benign task compliance), exposing tradeoffs.
Results indicate scaling alone and simple filtering are insufficient against skill-supply-chain injection.
Authors argue for context-aware authorization and stronger runtime governance around skill execution.

### Novelty
- First targeted benchmark for **skill-file** injection in agent ecosystems.
- Joint utility-security evaluation highlights defense tradeoffs, not just attack success.

### Security relevance
- **Data security:** demonstrates practical exfiltration paths through extension/skill supply chain.
- **Permissions:** motivates explicit per-skill capability bounds and provenance-aware policy checks.

### Limitations / open questions
- Coverage of non-textual skill artifacts (binary plugins, sandbox escapes) is limited.
- Transferability to enterprise proprietary agent stacks requires additional validation.

### Tags
`prompt-injection` `skills` `agent-supply-chain` `data-exfiltration` `benchmark` `authorization`

---

## 3) Right to History: A Sovereignty Kernel for Verifiable AI Agent Execution
- **Authors:** Jing Zhang
- **arXiv:** `2602.20214v1` (**cs.CR**, cs.AI, cs.OS)

### Summary
The paper proposes “Right to History”: users should receive verifiable logs of all local agent actions.
It formalizes five invariants and implements them in PunkGo, a Rust sovereignty kernel.
Core design combines capability isolation, Merkle-tree audit logging (RFC 6962), and human approvals.
It targets personal/on-device deployments where no central provider can be trusted as log authority.
Adversarial tests are used to evaluate whether invariants hold under hostile conditions.
Reported overhead is low (sub-1.3ms median action latency, ~400 actions/s, compact inclusion proofs).
The argument aligns technical logging architecture with emerging compliance pressure (e.g., high-risk AI logs).

### Novelty
- Rights-framed system invariants translated into concrete kernel-level enforcement.
- Practical tamper-evident action-history construction for local-first agent deployments.

### Security relevance
- **Permissions + audit:** capability isolation plus verifiable provenance closes post-incident ambiguity.
- **Data governance:** immutable action traces support accountability and regulatory evidence.

### Limitations / open questions
- Single-system prototype; ecosystem integration and migration path remain unclear.
- Human-approval UX and alert fatigue under high action volumes need study.

### Tags
`verifiable-logging` `capability-isolation` `audit-trail` `agent-governance` `sovereign-compute`

---

## 4) Towards Trustworthy GUI Agents: A Survey
- **Authors:** Yucheng Shi, Wenhao Yu, Jingyuan Huang, Wenlin Yao, Wenhu Chen, Ninghao Liu
- **arXiv:** `2503.23434v2` (**cs.LG**) — **version update in window**

### Summary
This survey centers GUI agents, where mistakes can trigger irreversible operations (delete/send/authorize).
It frames trust failures as an execution-gap across perception, reasoning, and interaction stages.
A workflow-aligned taxonomy maps benign failures and adversarial attacks stage by stage.
The authors catalog defensive methods specific to GUI environments rather than chat-only settings.
It argues task completion is insufficient as a sole metric for safe deployment.
Trust-aware evaluation needs process metrics for cascading errors and utility-security tradeoffs.
The synthesis is useful as a structured checklist for deployment governance and red-teaming coverage.

### Novelty
- Unified trust taxonomy tied to concrete GUI execution pipeline stages.
- Emphasis on process-level trust metrics over endpoint success only.

### Security relevance
- **Permissions:** GUI agents directly invoke privileged user actions; survey clarifies control points.
- **Data security:** attack surface includes stealth permission grants and unintended data disclosure flows.

### Limitations / open questions
- As a survey, lacks new controlled empirical defense benchmarks.
- Standardized trust metrics still immature and fragmented.

### Tags
`gui-agents` `trustworthiness` `permission-risk` `attack-taxonomy` `evaluation`

---

## 5) What Matters For Safety Alignment?
- **Authors:** Xing Li, Hui-Ling Zhen, Lihao Yin, Xianzhi Yu, Zhenhua Dong, Mingxuan Yuan
- **arXiv:** `2601.03868v2` (**cs.CL**, cs.AI, cs.CR) — **version update in window**

### Summary
A large-scale empirical study compares safety alignment across 32 LLM/LRM models.
It tests intrinsic factors and external attack styles using 5 datasets, 56 jailbreak methods, and CoT attacks.
The analysis indicates reasoning/self-reflection mechanisms correlate with stronger safety outcomes.
It reports safety regression risks from some post-training and distillation pipelines.
A critical result: response-prefix CoT attacks can massively increase attack success in some models.
The paper highlights deployment risks in text-completion interfaces allowing user-controlled prefixes.
Overall, it shifts focus from model size to training/interface choices that materially alter safety.

### Novelty
- Broad, systematic cross-model comparison with unusually large attack coverage.
- Strong evidence connecting interface affordances (prefix control) to alignment breakdown.

### Security relevance
- **Data/security controls:** identifies concrete serving-interface risks that should be policy-restricted.
- **Permissions by policy:** supports stronger output-guardrails and interface-level safety gating.

### Limitations / open questions
- Benchmark dependence may underrepresent real-world agent tool loops.
- Causality between specific training choices and failures still partly observational.

### Tags
`safety-alignment` `jailbreak` `prompt-injection` `interface-security` `evaluation`

---

## 6) BrowseComp-V3: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents
- **Authors:** Yuzhe Yang, Hanwen Xu, Ruoxi Sun, Zhida Li, Sizhe Tao, Yiheng Wang, Kaixiang Yang, Zhiyi Zeng, Jiawei Zhang, Xiao Zhang, Ji-Rong Wen
- **arXiv:** `2602.12876v2` (**cs.AI**) — **version update in window**

### Summary
BrowseComp-V3 builds a challenging benchmark for multimodal browsing/deep-search agents.
Tasks require multi-hop reasoning across text and visual evidence spread over multiple webpages.
A key design choice is verifiable public evidence to improve reproducibility and auditability.
Beyond final answers, it evaluates intermediate subgoals to expose process-level agent weaknesses.
The paper introduces OmniSeeker as a unified multimodal browsing-agent framework for experiments.
Reported performance ceilings remain low (~36% for SOTA models), indicating unresolved reliability gaps.
The benchmark is useful for evaluating safe browsing policies where evidence provenance matters.

### Novelty
- Verifiable-evidence requirement plus process-level scoring for multimodal web agents.
- Better diagnostic power than pure answer accuracy benchmarks.

### Security relevance
- **Data security/audit:** provenance-linked evidence tracing supports auditing of agent decisions.
- **Permissions:** helps test whether browsing agents overreach tools under uncertainty.

### Limitations / open questions
- Security attacks are not the primary benchmark objective.
- Coverage of adversarial web content/prompt-injection scenarios could be expanded.

### Tags
`browsing-agents` `multimodal` `verifiable-evidence` `process-evaluation` `benchmark`

---

## 7) A Secure and Interoperable Architecture for EHR Access Control and Sharing
- **Authors:** Tayeb Kenaza, Islam Debicha, Youcef Fares, Mehdi Sehaki, Sami Messai
- **arXiv:** `2602.20830v1` (**cs.CR**)

### Summary
This work proposes a blockchain+IPFS architecture for secure EHR access control and sharing.
Smart contracts on a private blockchain enforce patient-centric control over record access.
The design targets GDPR-style privacy compliance and interoperability with existing healthcare systems.
It addresses both access-control enforcement and heterogeneous data-sharing coordination.
Prototype implementation on Hyperledger is used to simulate multiple provider-sharing scenarios.
Results indicate scalability and operational feasibility within the tested setup.
Though not LLM-specific, the permissioning patterns are relevant to sensitive agentic data workflows.

### Novelty
- End-to-end patient-controlled governance model integrated with interoperable health systems.
- Practical prototype evidence for policy-enforced sharing in heterogeneous environments.

### Security relevance
- **Permissions/data governance:** clear policy-bound data access and auditable sharing workflows.
- Transferable design cues for agent systems handling regulated personal data.

### Limitations / open questions
- Generalization beyond healthcare domain is not directly demonstrated.
- Smart-contract governance assumptions (key management, emergency overrides) need deeper analysis.

### Tags
`access-control` `privacy` `data-governance` `blockchain` `ehr-security`

---

## 8) GPM: The Gaussian Pancake Mechanism for Planting Undetectable Backdoors in Differential Privacy
- **Authors:** Haochen Sun, Xi He
- **arXiv:** `2509.23834v2` (**cs.CR**, cs.DB) — **version update in window**

### Summary
The paper studies a supply-chain-style threat against differential privacy implementations.
It proposes GPM, designed to appear computationally indistinguishable from Gaussian mechanism behavior.
Despite that covertness, GPM can induce significantly weaker statistical privacy guarantees.
This turns implementation assumptions into an active privacy backdoor rather than accidental misconfig.
The authors provide formal analysis of covertness and leakage plus a concrete distinguishing attack.
Empirical/theoretical evidence suggests high attack success under suitable settings.
The work reinforces that privacy security depends on implementation verifiability, not just DP theory.

### Novelty
- Introduces an “undetectable” DP backdoor framing with formal indistinguishability-vs-leakage separation.
- Connects software trust to privacy guarantee integrity in a rigorous way.

### Security relevance
- **Data privacy:** direct implications for training/inference data governance pipelines using DP tooling.
- **Audit:** motivates reproducible builds and formal verification in privacy-critical agent infrastructure.

### Limitations / open questions
- Practical exploitability depends on software supply-chain insertion feasibility.
- Defensive detection strategies and operational mitigations need deeper empirical benchmarking.

### Tags
`differential-privacy` `backdoor` `privacy-leakage` `supply-chain-security` `formal-analysis`

---

## 9) ContextPilot: Fast Long-Context Inference via Context Reuse
- **Authors:** Yinsicheng Jiang, Yeqi Huang, Liang Cheng, Cheng Deng, Xuan Sun, Luo Mai
- **arXiv:** `2511.03475v3` (**cs.LG**) — **version update in window**

### Summary
ContextPilot targets long-context inference bottlenecks by maximizing reusable context blocks.
It introduces a context index to detect overlap across users/sessions/turns.
Additional ordering and de-duplication steps increase KV-cache reuse efficiency.
To protect quality, it adds compact context annotations when reuse might distort reasoning.
The architecture is modular and designed to integrate with existing inference engines.
Reported gains: up to 3× prefill speedup while maintaining, sometimes improving, quality.
For agent stacks with persistent memory, this is an operationally relevant memory-access optimization.

### Novelty
- Practical context-index abstraction for cross-interaction reuse.
- Quality-preserving reuse strategy via annotation rather than brute pruning.

### Security relevance
- **Memory access dimension:** changes what historical context is reused across requests, impacting isolation boundaries.
- Raises governance questions on cross-session data contamination and tenant separation policies.

### Limitations / open questions
- Privacy leakage risk from cross-user reuse needs explicit threat-model analysis.
- Benefits may vary heavily by workload overlap and deployment architecture.

### Tags
`long-context` `memory-reuse` `kv-cache` `inference-systems` `context-isolation`

---

## 10) CHESS: Context-aware Hierarchical Efficient Semantic Selection for Long-Context LLM Inference
- **Authors:** Chao Fei, Guozhong Li, Chenxi Liu, Panos Kalnis
- **arXiv:** `2602.20732v1` (**cs.AI**)

### Summary
CHESS proposes a hierarchical, context-aware KV-cache management method for long-context decoding.
Unlike context-agnostic pruning, it reconstructs context dynamically based on step-wise relevance.
The system side uses coarse-grained selection to reduce irregular data movement overhead.
This algorithm-system co-design aims to convert theoretical sparsity into real latency gains.
Reportedly, CHESS can exceed full-KV quality while using only ~1% of cache in tested settings.
Throughput gains up to ~4.56× are reported against strong baselines.
The paper is primarily efficiency-focused but directly intersects with memory lifecycle controls.

### Novelty
- Joint semantic selection + systems design for practical sparse KV management.
- Demonstrates aggressive cache reduction without apparent quality collapse in evaluations.

### Security relevance
- **Memory lifecycle:** impacts retention/selection of contextual state that can contain sensitive traces.
- Suggests need for policy-aware cache eviction and context provenance tagging.

### Limitations / open questions
- Robustness under adversarial or poisoned context distributions is not evaluated.
- Security/privacy-aware cache policies are outside scope.

### Tags
`kv-cache` `long-context` `memory-lifecycle` `llm-inference` `systems`

---

## 11) Multi-Vector Index Compression in Any Modality
- **Authors:** Hanxiang Qin, Alexander Martin, Rohan Jha, Chunsheng Zuo, Reno Kriz, Benjamin Van Durme
- **arXiv:** `2602.21202v1` (**cs.IR**, cs.CL, cs.CV)

### Summary
This paper studies index compression for late-interaction retrieval across text, vision, and video.
It compares sequence resizing, memory tokens, hierarchical pooling, and attention-guided clustering (AGC).
AGC picks salient regions as cluster centroids and uses attention-weighted aggregation.
Benchmarks across BEIR, ViDoRe, MSR-VTT, and MultiVENT show AGC is generally strongest.
The method achieves competitive retrieval quality under strict vector budgets.
It addresses storage/computation scaling limits central to large retrieval-augmented systems.
While not security-centric, it is relevant to memory indexing strategy for agent knowledge stores.

### Novelty
- Modality-agnostic compression study with a new attention-guided clustering method.
- Flexible index-budget control without large quality loss.

### Security relevance
- **Memory indexing/access:** compressed indexes affect what evidence is retrievable during agent decisions.
- Can indirectly alter safety by changing recall fidelity for policy or sensitive-document retrieval.

### Limitations / open questions
- Security-aware retrieval quality (e.g., policy-doc recall under compression) is untested.
- Tradeoffs under adversarially crafted documents are unclear.

### Tags
`retrieval` `index-compression` `memory-indexing` `multimodal-ir` `late-interaction`

---

## 12) Towards Efficient Agents: A Co-Design of Inference Architecture and System
- **Authors:** Yicheng Zhang, Zixuan Wang, Guangyao Dou, Shiqi Song, Qihao Wang, Yuxiang Wu, Xiangyu Zhang, Wei Bi, Shusen Wang
- **arXiv:** `2512.18337v2` (**cs.CL**) — **version update in window**

### Summary
AgentInfer reframes performance optimization at end-to-end agent task level, not token-level throughput.
It includes four modules: dual-model reasoning roles, hybrid scheduling, speculative decoding reuse, and memory compression.
A “Self-Evolution Engine” is proposed to maintain efficiency over long-horizon reasoning loops.
Benchmarks report 1.8–2.5× speedups with >50% reduction in ineffective token usage.
The approach highlights orchestration-level efficiency as a first-class systems objective.
Its memory compression and multi-session reuse components are especially relevant to agent memory lifecycle design.
Though not a security paper, it surfaces architecture choices that can influence access boundaries and traceability.

### Novelty
- Integrated inference+architecture co-design specific to multi-turn tool-using agents.
- Emphasis on sustained long-horizon efficiency rather than isolated model acceleration.

### Security relevance
- **Memory/access:** cross-session reuse and compression can affect confidentiality and forensic clarity.
- **Policy:** scheduler/role assignment may need explicit privilege partitioning in secure deployments.

### Limitations / open questions
- Security impacts of memory reuse/compression are not evaluated.
- Results may be benchmark-dependent and deployment-specific.

### Tags
`agent-systems` `memory-compression` `scheduler` `long-horizon` `architecture`

---

## Top 3 must-read + recommended reading order

1. **OpenPort Protocol (2602.20196v1)**  
   Start here for concrete, protocol-level treatment of least privilege, write-risk gating, TOCTOU checks, and audit semantics.

2. **Skill-Inject (2602.20156v2)**  
   Then read the attack evidence showing why those governance controls are urgently needed in real skill ecosystems.

3. **Right to History (2602.20214v1)**  
   Finish with verifiable execution-history architecture: it complements prevention with post-hoc accountability and compliance readiness.

## 10-line trend summary
1. The strongest movement is from “prompt-level guardrails” toward **protocolized authorization and policy enforcement** for agent tools.  
2. Tool/skill supply-chain risk is now empirically clear; injection resistance cannot rely on scaling or static filtering alone.  
3. Auditability is becoming first-class: tamper-evident logs and deterministic failure semantics are moving into core designs.  
4. Interface choices (e.g., response-prefix affordances) materially alter safety outcomes, not just model weights.  
5. GUI/browsing agents keep expanding irreversible action surfaces, raising pressure for stage-wise trust metrics.  
6. Memory/inference papers increasingly optimize context/index lifecycle, which will shape practical security boundaries.  
7. Cross-session context reuse and compressed memory layers introduce latent confidentiality and provenance risks if unmanaged.  
8. Data governance work outside LLMs (EHR, DP backdoor studies) offers transferable controls for regulated agent deployments.  
9. **Research gap #1:** unified benchmarks that jointly measure utility, authorization correctness, and exfiltration resistance in tool-using agents.  
10. **Research gap #2:** policy-aware memory systems (cache/index/reuse) with explicit tenant isolation and auditable provenance guarantees.
